{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datatable as dt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import validation_curve\n",
    "import fastai\n",
    "from fastai import *\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as cv\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = pd.read_csv(\"/kaggle/input/brazilian-ecommerce/olist_order_items_dataset.csv\")\n",
    "orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(orders_df.seller_id))   # there are 3095 sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity_df = orders_df.groupby(['order_id', 'product_id','seller_id','price'])['order_item_id'].sum().reset_index()\n",
    "#quantity_df = orders_df.groupby(['order_id', 'product_id','seller_id','price'])['order_item_id'].agg({\"quantity\":\"max\"}).reset_index()\n",
    "quantity_df['order_price'] = quantity_df['price']*quantity_df['order_item_id']\n",
    "quantity_df\n",
    "\n",
    "# get the revennue for each seller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple product quantity summary\n",
    "tdf= quantity_df.order_item_id.value_counts().sort_values()\n",
    "tdf = pd.DataFrame({'order_item_id':tdf.index, 'order_counts':tdf.values}).sort_values(\"order_counts\",ascending=False) \n",
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity_df[quantity_df[\"order_item_id\"]>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregating total revenue per seller\n",
    "total_revenue_df = quantity_df.groupby(['seller_id'])['order_price'].agg('sum').reset_index()\n",
    "total_revenue_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read another dataset about all the order information, clean the data\n",
    "olist = pd.read_csv(\"../input/merge-all-olist/olist.csv\")\n",
    "olist = olist.drop(columns = 'Unnamed: 0')\n",
    "olist = olist.loc[:,[\"seller_id\",\"review_score\",\"price\", \"freight_value\"]]\n",
    "olist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the merged dataset abouut the sellers revenue information\n",
    "total_revenue_df = pd.DataFrame(olist.merge(total_revenue_df, on = \"seller_id\", how=\"left\").groupby(\"seller_id\")[\"order_price\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(olist[\"review_score\"]):\n",
    "    if (item == \"MG\") or (item == \"SP\"):\n",
    "        olist[\"review_score\"][index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(olist[\"price\"]):\n",
    "    if (item == \"PR\") or (item == \"SP\"):\n",
    "        olist[\"price\"][index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(olist[\"freight_value\"]):\n",
    "    if (item == \"health_beauty\") or (item == \"SP\") or (item == \"perfumery\"):\n",
    "        olist[\"freight_value\"][index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olist[\"review_score\"] = pd.to_numeric(olist.review_score)\n",
    "olist[\"price\"] = pd.to_numeric(olist.price)\n",
    "olist[\"freight_value\"] = pd.to_numeric(olist.freight_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge the olist dataset with total revenue dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olist = olist.groupby(\"seller_id\")[[\"review_score\", \"price\",\"freight_value\"]].sum()\n",
    "olist = olist.merge(total_revenue_df, on =\"seller_id\", how = \"inner\")\n",
    "olist = olist.rename(columns = {\"order_price\":\"revenue\"})\n",
    "olist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions on revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = [\"review_score\", \"price\",\"freight_value\"]\n",
    "X = olist[feature]\n",
    "y = olist.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state= 42,\n",
    "    #stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a regression evaluation function\n",
    "def rr(y_true, y_pred, model_name):\n",
    "    import pandas as pd\n",
    "    import sklearn.metrics as metrics\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)    \n",
    "    l = [{'explained_variance' : round(explained_variance,4),\n",
    "                       'mean_squared_log_error' : round(mean_squared_log_error,4),\n",
    "                       'r2' : round(r2,4),\n",
    "                       'MAE' : round(mean_absolute_error,4),\n",
    "                       'MSE' : round(mse,4),\n",
    "                       'RMSE' : round(np.sqrt(mse), 4)}]\n",
    "    df = pd.DataFrame(l).T\n",
    "    df = df.apply(lambda x: '%.5f' % x, axis = 1)\n",
    "    df = pd.DataFrame(df, columns = [model_name])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators= [100,200,300]\n",
    "max_depth = [3,4,5,6,10]\n",
    "# Create the random grid\n",
    "grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "}\n",
    "m =  RandomForestRegressor()\n",
    "mrf = RandomizedSearchCV(estimator = m, \n",
    "                              param_distributions = grid, \n",
    "                              n_iter = 20, \n",
    "                              cv = 3, \n",
    "                              verbose=2, \n",
    "                              random_state= 42, \n",
    "                              scoring='neg_root_mean_squared_error',\n",
    "                              n_jobs = -1)\n",
    "mrf.fit(X_train, y_train)\n",
    "print(\"score: \", mrf.best_score_)\n",
    "print(\"best estimator parameters: \", mrf.best_estimator_.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predrf = mrf.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators= [100,200,300]\n",
    "max_depth = [3,4,5,6,10]\n",
    "learning_rate = [0.03,0.3]\n",
    "subsample = [0.5,0.7,1]\n",
    "# Create the random grid\n",
    "grid = {'n_estimators': n_estimators,\n",
    "    'max_depth': max_depth,\n",
    "                'learning_rate' : learning_rate,\n",
    "                'subsample' : subsample,\n",
    "}\n",
    "mxgb = XGBRegressor()\n",
    "m_randomxgb = RandomizedSearchCV(estimator = mxgb, \n",
    "                              param_distributions = grid, \n",
    "                              n_iter = 90, \n",
    "                              cv = 3, \n",
    "                              verbose=2, \n",
    "                              random_state= 42, \n",
    "                              scoring='neg_root_mean_squared_error',\n",
    "                              n_jobs = -1)\n",
    "m_randomxgb.fit(X_train, y_train)\n",
    "print(m_randomxgb.best_params_)\n",
    "del m_randomxgb\n",
    "del mxgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxgb = xgb.XGBRegressor(\n",
    "    n_estimators = 300,\n",
    "    max_depth = 5 ,\n",
    "    learning_rate= 0.03,\n",
    "    subsample= 0.5 ,\n",
    "    random_state= 42,\n",
    "    tree_method='gpu_hist',\n",
    "    n_jobs = -1\n",
    ")\n",
    "mxgb.fit(X_train,y_train)\n",
    "y_predxgb = mxgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "mlr = LinearRegression(n_jobs = -1)\n",
    "mlr.fit(X_train, y_train)\n",
    "y_predlr = mlr.predict(X_test)\n",
    "for index, item in enumerate(y_predlr):\n",
    "    if item < 0:\n",
    "        y_predlr[index] = 0\n",
    "y_predlr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "n_estimators= [100,200,300]\n",
    "max_depth = [3,4,5,6,10]\n",
    "learning_rate = [0.03,0.3]\n",
    "subsample = [0.5,0.7,1]\n",
    "# Create the random grid\n",
    "grid = {'n_estimators': n_estimators,\n",
    "    'max_depth': max_depth,\n",
    "                'learning_rate' : learning_rate,\n",
    "                'subsample' : subsample,\n",
    "}\n",
    "mgb = GradientBoostingRegressor()\n",
    "m_randomgb = RandomizedSearchCV(estimator = mgb, \n",
    "                              param_distributions = grid, \n",
    "                              n_iter = 90, \n",
    "                              cv = 3, \n",
    "                              verbose=2, \n",
    "                              random_state= 42, \n",
    "                              scoring='neg_root_mean_squared_error',\n",
    "                              n_jobs = -1)\n",
    "m_randomgb.fit(X_train, y_train)\n",
    "y_predgb = m_randomgb.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(y_predgb):\n",
    "    if item < 0:\n",
    "        y_predgb[index] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bagging = 0.2 * y_predrf + 0.4 * y_predxgb + 0.1 * y_predlr + 0.3 * y_predgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "params = {\n",
    "    'boosting_type': 'gbdt', \n",
    "    'objective': 'regression', \n",
    "\n",
    "    'learning_rate': 0.1, \n",
    "    'num_leaves': 50, \n",
    "    'max_depth': 6,\n",
    "\n",
    "    'subsample': 0.8, \n",
    "    'colsample_bytree': 0.8, \n",
    "    }\n",
    "data_train = lgb.Dataset(X_train, y_train, silent=True)\n",
    "cv_results = lgb.cv(\n",
    "    params, data_train, num_boost_round=1000, nfold=5, stratified=False, shuffle=True, metrics='rmse',\n",
    "    early_stopping_rounds=50, verbose_eval=50, show_stdv=True, seed=0)\n",
    "\n",
    "print('best n_estimators:', len(cv_results['rmse-mean']))\n",
    "print('best cv score:', cv_results['rmse-mean'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=50,\n",
    "                              learning_rate=0.1, n_estimators=9, max_depth=6,\n",
    "                              metric='rmse', bagging_fraction = 0.8,feature_fraction = 0.8)\n",
    "\n",
    "params_test1={\n",
    "    'max_depth': range(3,8,2),\n",
    "    'num_leaves':range(50, 170, 30)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator=model_lgb, param_grid=params_test1, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "gsearch1.fit(df_train, y_train)\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_test2={\n",
    "    'max_depth': [6,7,8],\n",
    "    'num_leaves':[38,44,50,56,62]\n",
    "}\n",
    "\n",
    "gsearch2 = GridSearchCV(estimator=model_lgb, param_grid=params_test2, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "gsearch2.fit(df_train, y_train)\n",
    "gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_test3={\n",
    "    'min_child_samples': [18, 19, 20, 21, 22],\n",
    "    'min_child_weight':[0.001, 0.002]\n",
    "}\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=50,\n",
    "                              learning_rate=0.1, n_estimators=9, max_depth=7, \n",
    "                              metric='rmse', bagging_fraction = 0.8, feature_fraction = 0.8)\n",
    "gsearch3 = GridSearchCV(estimator=model_lgb, param_grid=params_test3, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "gsearch3.fit(df_train, y_train)\n",
    "gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_test4={\n",
    "    'feature_fraction': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=50,\n",
    "                              learning_rate=0.1, n_estimators=9, max_depth=7, \n",
    "                              metric='rmse', bagging_freq = 5,  min_child_samples= 19)\n",
    "gsearch4 = GridSearchCV(estimator=model_lgb, param_grid=params_test4, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "gsearch4.fit(df_train, y_train)\n",
    "gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_test5={\n",
    "    'feature_fraction': [0.82, 0.85, 0.88, 0.9, 0.92, 0.95, 0.98 ]\n",
    "}\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves= 50,\n",
    "                              learning_rate=0.1, n_estimators= 9, max_depth=7, \n",
    "                              metric='rmse',  min_child_samples=19)\n",
    "gsearch5 = GridSearchCV(estimator=model_lgb, param_grid=params_test5, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "gsearch5.fit(df_train, y_train)\n",
    "gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_test6={\n",
    "    'reg_alpha': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5],\n",
    "    'reg_lambda': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5]\n",
    "}\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=50,\n",
    "                              learning_rate=0.1, n_estimators=9, max_depth=7, \n",
    "                              metric='rmse',  min_child_samples=19, feature_fraction=0.85)\n",
    "gsearch6 = GridSearchCV(estimator=model_lgb, param_grid=params_test6, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "gsearch6.fit(df_train, y_train)\n",
    "gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt', \n",
    "    'objective': 'regression', \n",
    "\n",
    "    'learning_rate': 0.005, \n",
    "    'num_leaves': 50, \n",
    "    'max_depth': 9,\n",
    "    'min_data_in_leaf': 19,\n",
    "\n",
    "    'subsample': 1, \n",
    "    'colsample_bytree': 0.85, \n",
    "    }\n",
    "\n",
    "data_train = lgb.Dataset(df_train, y_train, silent=True)\n",
    "cv_results = lgb.cv(\n",
    "    params, data_train, num_boost_round=10000, nfold=5, stratified=False, shuffle=True, metrics='rmse',\n",
    "    early_stopping_rounds=50, verbose_eval=100, show_stdv=True)\n",
    "\n",
    "print('best n_estimators:', len(cv_results['rmse-mean']))\n",
    "print('best cv score:', cv_results['rmse-mean'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = lgb.LGBMRegressor(**params, n_estimators = 267)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predlgb = lg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpred = rr(y_test, y_predrf, \"RandomForestRegressor\")\n",
    "xgbpred  = rr(y_test, y_predxgb, \"XGBoostRegressor\")\n",
    "lrpred = rr(y_test, y_predlr, \"LinearRegression\")\n",
    "gbpred = rr(y_test, y_predgb, \"GradientBoostingRegressor\")\n",
    "baggingpred = rr(y_test, y_bagging, \"0.2 * RF + 0.4 * XGB + 0.1 * LR + 0.3 * GB\")\n",
    "df = pd.concat([rfpred, xgbpred, lrpred, gbpred, baggingpred], axis= 1).T\n",
    "df.reset_index(inplace=True)\n",
    "df = df.rename(columns = {'index':'model'})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,item in enumerate(y_bagging):\n",
    "    y_bagging[index] = round(item, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1.357e-02\n",
    "x = float(str(x))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,item in enumerate(y_bagging):\n",
    "    y_bagging[index] = round(item, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"test_data\": y_test, \"bagging_result\": y_bagging}).iloc[[104,116,118,2,204,306,341,335,334,333],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,1:7] = df.iloc[:,1:7].apply(lambda x: pd.to_numeric(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(20,8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data = df, x=\"model\", y='RMSE', hue = \"model\", order = df.sort_values('RMSE').model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = pd.DataFrame({\"y_test\" : y_test, \"y_predxgb\": y_predxgb})\n",
    "check[\"y_predxgb\"] = check.y_predxgb.apply(lambda x: round(x,2))\n",
    "check.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close = pd.read_csv(\"/kaggle/input/marketing-funnel-olist/olist_closed_deals_dataset.csv\")\n",
    "# marketing = pd.read_csv(\"/kaggle/input/marketing-funnel-olist/olist_marketing_qualified_leads_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funnel_df = marketing.merge(close, on = \"mql_id\", how = \"left\")\n",
    "# funnel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #merging funnel df and total revenue df (379 out of 841 leads have seller data)\n",
    "# funnel_df.first_contact_date = pd.to_datetime(funnel_df.first_contact_date) #normalizing contact date\n",
    "# funnel_df[\"contact_month\"] = funnel_df.first_contact_date.dt.month\n",
    "# final_df_left = funnel_df.merge(total_revenue_df, on='seller_id', how=\"left\")\n",
    "# final_df_inner = funnel_df.merge(total_revenue_df, on='seller_id', how=\"inner\")\n",
    "\n",
    "\n",
    "# final_df_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_imp = pd.Series(rf_model.feature_importances_,index=features).sort_values(ascending=False)\n",
    "# feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olist data['english'] = data['review_comment_message'].apply(translator.translate, src='pt', dest='en').apply(getattr, args=('text',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "#from google_trans_new import google_translator  \n",
    "#translator = google_translator() \n",
    "\n",
    "from googletrans import Translator\n",
    "\n",
    " \n",
    "review = pd.read_csv(\"/kaggle/input/brazilian-ecommerce/olist_order_reviews_dataset.csv\")\n",
    "t = review.review_comment_message.fillna(\"no comment\")\n",
    "t = t.drop(list(range(2171)))\n",
    "t = t.drop(list(range(2171,3932)))\n",
    "t = t.drop(list(range(3932,4342)))\n",
    "t = t.drop(list(range(4342,5342)))\n",
    "t = t.drop(list(range(5342,6513)))  \n",
    "t = t.drop(list(range(6513,7513)))\n",
    "t = t.drop(list(range(7513,8684)))  #1171\n",
    "t = t.drop(list(range(8684, 10000)))  #1316\n",
    "t = t.drop(list(range(10000, 12297)))\n",
    "t = t.drop(list(range(12297, 17156)))\n",
    "t = t.drop(list(range(17156, 17406)))\n",
    "t = t.drop(list(range(17406, 27181)))\n",
    "# english = []\n",
    "# for i in t.head(1316):\n",
    "#     english.append(translator.translate(i, lang_tgt='en' ))\n",
    "# pd.DataFrame(english).to_csv('english.csv', index= False)\n",
    "t = t.reset_index().drop(columns = \"index\")\n",
    "t = t.review_comment_message.values\n",
    "for index, item in enumerate(t):\n",
    "    if (\"/r\" in item) or (\"/n\" in item) or (\"\\n\" in item) or (\"\\r\" in item):\n",
    "        t[index] = \"no comment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = []\n",
    "translator = Translator()\n",
    "for i in t:\n",
    "    english.append(translator.translate(i).text)\n",
    "    time.sleep(0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(english).to_csv('english.csv', index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
